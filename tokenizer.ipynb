{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "578d838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap g\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Dursley, of number\n"
     ]
    }
   ],
   "source": [
    "# Store the book in a variable raw_text\n",
    "with open(\"books_text/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "with open(\"books_text/sorcerers-stone.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hp1_sorcerers_stone_text = f.read()\n",
    "print(raw_text[:50])\n",
    "print(hp1_sorcerers_stone_text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccbf820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n",
      "['THE', 'BOY', 'WHO', 'LIVED', 'Mr', '.', 'and', 'Mrs', '.', 'Dursley', ',', 'of', 'number', 'four', ',', 'Privet', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.', 'They', 'were', 'the', 'last', 'people', 'you’d', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', ',', 'because']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Use regex to get split by punctuation\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "# Get rid of the white spaces. White spaces are not needed for this use case\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "hp1_preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', hp1_sorcerers_stone_text)\n",
    "hp1_preprocessed = [item.strip() for item in hp1_preprocessed if item.strip()]\n",
    "\n",
    "print(preprocessed[:50])\n",
    "print(hp1_preprocessed[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47df964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "# Create the Token IDs\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f307fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID is 0 and item is !\n",
      "ID is 1 and item is \"\n",
      "ID is 2 and item is '\n",
      "ID is 3 and item is (\n",
      "ID is 4 and item is )\n",
      "ID is 5 and item is ,\n",
      "ID is 6 and item is --\n",
      "ID is 7 and item is .\n",
      "ID is 8 and item is :\n",
      "ID is 9 and item is ;\n",
      "ID is 10 and item is ?\n"
     ]
    }
   ],
   "source": [
    "vocab = {token: integer for integer,token in enumerate(all_words)}\n",
    "for token,i in vocab.items():\n",
    "    print(f'ID is {i} and item is {token}')\n",
    "    if i == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "# Create Tokenizer class\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: item for item,i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "# Test out the Tokenizer class, and the encode method\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d45b21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out the decode method\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83788b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      9\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     11\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "# If tokens are not properly mapped, an error will be thrown since in this case, \n",
    "# Hello was not in the pretrained data\n",
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b3aa585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "7388\n"
     ]
    }
   ],
   "source": [
    "# <|unk|> and <|endoftext|> can be used when,\n",
    "# Use <|unk|> when tokenizer encounters an unknown word (isn't in vocab)\n",
    "# Use <|endoftext|> to signifiy the end of a text \n",
    "# Ex. When we finish reading article 1, we put <|endoftext|>\n",
    "# At the start of article 2, and so on\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "all_tokens_hp1 = sorted(list(set(hp1_preprocessed)))\n",
    "all_tokens_hp1.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token: i for i,token in enumerate(all_tokens)}\n",
    "vocab_hp1 = {token: i for i,token in enumerate(all_tokens_hp1)}\n",
    "\n",
    "print(len(vocab))\n",
    "print(len(vocab_hp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cadbcc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('younger', 1127), 0)\n",
      "(('your', 1128), 1)\n",
      "(('yourself', 1129), 2)\n",
      "(('<|endoftext|>', 1130), 3)\n",
      "(('<|unk|>', 1131), 4)\n",
      "--------\n",
      "(('”', 7383), 0)\n",
      "(('•k', 7384), 1)\n",
      "(('■”', 7385), 2)\n",
      "(('<|endoftext|>', 7386), 3)\n",
      "(('<|unk|>', 7387), 4)\n"
     ]
    }
   ],
   "source": [
    "# As you can see, the two special tokens are present at the bottom\n",
    "for i,item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(f'({item}, {i})')\n",
    "print(\"--------\")\n",
    "for i,item in enumerate(list(vocab_hp1.items())[-5:]):\n",
    "    print(f'({item}, {i})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abfc8be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: token for token,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)           \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        # If token is not present in our current vocab, replace it by an unknown token\n",
    "        preprocessed = [\n",
    "            token if token in self.str_to_int \n",
    "            else \"<|unk|>\" for token in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace\"\n",
    "# Append the <|endoftext|> at the end of text 1 and beginning\n",
    "# of text 2\n",
    "text = \" <|endoftext|> \".join((text1,text2))\n",
    "print(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd64ea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace\n"
     ]
    }
   ],
   "source": [
    "hp1_tokenizer = SimpleTokenizerV2(vocab_hp1)\n",
    "text1 = \"Percy Jackson and the Lightning Thief\"\n",
    "text2 = \"Hello. It's me. I was wondering if after all...\"\n",
    "\n",
    "text_hp = \" <|endoftext|> \".join((text1,text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8afac624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1131 is the token ID of \"<|unk|>\" and 1130 is the \n",
    "# token ID of <|endoftext|>\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c920f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[849,\n",
       " 7387,\n",
       " 1380,\n",
       " 6013,\n",
       " 7387,\n",
       " 1105,\n",
       " 7386,\n",
       " 7387,\n",
       " 7,\n",
       " 587,\n",
       " 2,\n",
       " 5058,\n",
       " 4058,\n",
       " 7,\n",
       " 568,\n",
       " 6452,\n",
       " 6641,\n",
       " 3604,\n",
       " 1328,\n",
       " 1354,\n",
       " 7,\n",
       " 7,\n",
       " 7]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp1_tokenizer.encode(text_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a88fa52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "622fce28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Percy <|unk|> and the <|unk|> Thief <|endoftext|> <|unk|>. It' s me. I was wondering if after all...\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp1_tokenizer.decode(hp1_tokenizer.encode(text_hp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30071b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From comparing the decoded tokenized text above with \n",
    "# The original input text, we know that the training set \n",
    "# did not contain the words \"Hello\" and \"palace\"\n",
    "\n",
    "# Could use other tokens\n",
    "# [BOS] (beginning of sequence) Marks the start of a text/sequence\n",
    "# [EOS] (end of sequence): Positioned at the end of a text\n",
    "# and is useful for concatenating multiple unrelated texts.\n",
    "# [PAD] (padding): When training LLMs with a batch size larger\n",
    "# than one, the batch might contain texts of varying lengths. To ensure that \n",
    "# all the texts have the smae length the shorter texts\n",
    "# are padded using [PAD] token, up to length of the longest text in batch\n",
    "\n",
    "# Tokenizer used for GPT models does not need any of these \n",
    "# tokens mentioned but only uses an <|endoftext|> for simplicity\n",
    "\n",
    "# byte pair enconding tokenizer breaks down words \n",
    "# into subword units for tokenizer with GPT models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f0ae6",
   "metadata": {},
   "source": [
    "#### BYTE PAIR ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "619379d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.11.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Downloading tiktoken-0.11.0-cp312-cp312-macosx_11_0_arm64.whl (996 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.7/996.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.11.0\n"
     ]
    }
   ],
   "source": [
    "# Use tiktoken github\n",
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d52ccf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(f'tiktoken version: {importlib.metadata.version(\"tiktoken\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b44d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate BPE tokenizer from tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "hp1_tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3e9f709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271]\n"
     ]
    }
   ],
   "source": [
    "# Usage of tokenizer is similar to SimpleTokenzizerV2\n",
    "# We implemented previously via an encode method\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace\"\n",
    ")\n",
    "encoded_mapping = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
    "print(encoded_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58c72e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace\n"
     ]
    }
   ],
   "source": [
    "# Can then convert token IDs back into text using the decode\n",
    "# method. Similar to the SimpleTokenizerV2\n",
    "# Tokenizer is able to encode words that look wrong, like \n",
    "# someunknownPlace\n",
    "\n",
    "tokens = tokenizer.decode(encoded_mapping)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d137f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two advantages so far.\n",
    "# 1. Reduces the amount of tokens we have\n",
    "# 2. Can deal with unknown words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6111628",
   "metadata": {},
   "source": [
    "### Can make two noteworthy observations based on the token IDs and decoded text above:\n",
    "\n",
    "#### 1. <|endoftext|> token is assigned to relatively large token ID (50256)\n",
    "#### 2. BPE tokenizer above encodes and decodes unknown words, can handle any unknown word.\n",
    "\n",
    "#### Algorithm underlying BPE breaks down words that aren't in its predefined vocab into smaller subwords. \n",
    "#### This enables it to handle out-of-vocabulary words.\n",
    "#### Because of BPE algorithm, if the tokenizer encounters an unfamilar word during tokenization, it can represent it as a sequence of subwords or characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af08c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 80, 220, 959]\n",
      "Akwirq ier\n"
     ]
    }
   ],
   "source": [
    "# Proof that it works, even with gibberish\n",
    "encoded_mapping = tokenizer.encode(\"Akwirq ier\")\n",
    "print(encoded_mapping)\n",
    "\n",
    "tokens = tokenizer.decode(encoded_mapping)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317ada9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 1047, 11, 9457, 11, 300, 1047, 11, 277, 1047, 11, 17666, 338, 11, 460, 470, 11, 6584, 470, 11, 460, 470, 11, 1839, 470, 11, 1028, 11, 22279, 11, 1276, 11, 17000, 11, 34297, 11, 427, 272, 470]\n",
      "tames, breaks, lames, fames, dont's, can't, shouldn't, can't, won't, against, lust, must, rust, fuss, shan't\n"
     ]
    }
   ],
   "source": [
    "# One more example with words that include apostrophes\n",
    "text = \"tames, breaks, lames, fames, dont's, can't, shouldn't, can't, won't, against, lust, must, rust, fuss, shan't\"\n",
    "encoded_mapping = tokenizer.encode(text)\n",
    "print(encoded_mapping)\n",
    "\n",
    "tokens = tokenizer.decode(encoded_mapping)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33698f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
