{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578d838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap g\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Dursley, of number\n"
     ]
    }
   ],
   "source": [
    "# Store the book in a variable raw_text\n",
    "with open(\"books_text/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "with open(\"books_text/sorcerers-stone.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hp1_sorcerers_stone_text = f.read()\n",
    "print(raw_text[:50])\n",
    "print(hp1_sorcerers_stone_text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbf820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n",
      "['THE', 'BOY', 'WHO', 'LIVED', 'Mr', '.', 'and', 'Mrs', '.', 'Dursley', ',', 'of', 'number', 'four', ',', 'Privet', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.', 'They', 'were', 'the', 'last', 'people', 'you’d', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', ',', 'because']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Use regex to get split by punctuation\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "# Get rid of the white spaces. White spaces are not needed for this use case\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "hp1_preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', hp1_sorcerers_stone_text)\n",
    "hp1_preprocessed = [item.strip() for item in hp1_preprocessed if item.strip()]\n",
    "\n",
    "print(preprocessed[:50])\n",
    "print(hp1_preprocessed[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47df964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "# Create the Token IDs\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f307fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID is 0 and item is !\n",
      "ID is 1 and item is \"\n",
      "ID is 2 and item is '\n",
      "ID is 3 and item is (\n",
      "ID is 4 and item is )\n",
      "ID is 5 and item is ,\n",
      "ID is 6 and item is --\n",
      "ID is 7 and item is .\n",
      "ID is 8 and item is :\n",
      "ID is 9 and item is ;\n",
      "ID is 10 and item is ?\n"
     ]
    }
   ],
   "source": [
    "vocab = {token: integer for integer,token in enumerate(all_words)}\n",
    "for token,i in vocab.items():\n",
    "    print(f'ID is {i} and item is {token}')\n",
    "    if i == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5332edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "# Create Tokenizer class\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: item for item,i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "# Test out the Tokenizer class, and the encode method\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d45b21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out the decode method\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df83788b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# If tokens are not properly mapped, an error will be thrown since in this case, \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Hello was not in the pretrained data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      9\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     11\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "# If tokens are not properly mapped, an error will be thrown since in this case, \n",
    "# Hello was not in the pretrained data\n",
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3aa585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "7388\n"
     ]
    }
   ],
   "source": [
    "# <|unk|> and <|endoftext|> can be used when,\n",
    "# Use <|unk|> when tokenizer encounters an unknown word (isn't in vocab)\n",
    "# Use <|endoftext|> to signifiy the end of a text \n",
    "# Ex. When we finish reading article 1, we put <|endoftext|>\n",
    "# At the start of article 2, and so on\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "all_tokens_hp1 = sorted(list(set(hp1_preprocessed)))\n",
    "all_tokens_hp1.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token: i for i,token in enumerate(all_tokens)}\n",
    "vocab_hp1 = {token: i for i,token in enumerate(all_tokens_hp1)}\n",
    "\n",
    "print(len(vocab))\n",
    "print(len(vocab_hp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cadbcc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('younger', 1127), 0)\n",
      "(('your', 1128), 1)\n",
      "(('yourself', 1129), 2)\n",
      "(('<|endoftext|>', 1130), 3)\n",
      "(('<|unk|>', 1131), 4)\n",
      "--------\n",
      "(('”', 7383), 0)\n",
      "(('•k', 7384), 1)\n",
      "(('■”', 7385), 2)\n",
      "(('<|endoftext|>', 7386), 3)\n",
      "(('<|unk|>', 7387), 4)\n"
     ]
    }
   ],
   "source": [
    "# As you can see, the two special tokens are present at the bottom\n",
    "for i,item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(f'({item}, {i})')\n",
    "print(\"--------\")\n",
    "for i,item in enumerate(list(vocab_hp1.items())[-5:]):\n",
    "    print(f'({item}, {i})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abfc8be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: token for token,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)           \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        # If token is not present in our current vocab, replace it by an unknown token\n",
    "        preprocessed = [\n",
    "            token if token in self.str_to_int \n",
    "            else \"<|unk|>\" for token in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace\"\n",
    "# Append the <|endoftext|> at the end of text 1 and beginning\n",
    "# of text 2\n",
    "text = \" <|endoftext|> \".join((text1,text2))\n",
    "print(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd64ea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace\n"
     ]
    }
   ],
   "source": [
    "hp1_tokenizer = SimpleTokenizerV2(vocab_hp1)\n",
    "text1 = \"Percy Jackson and the Lightning Thief\"\n",
    "text2 = \"Hello. It's me. I was wondering if after all...\"\n",
    "\n",
    "text_hp = \" <|endoftext|> \".join((text1,text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8afac624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1131 is the token ID of \"<|unk|>\" and 1130 is the \n",
    "# token ID of <|endoftext|>\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c920f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[849,\n",
       " 7387,\n",
       " 1380,\n",
       " 6013,\n",
       " 7387,\n",
       " 1105,\n",
       " 7386,\n",
       " 7387,\n",
       " 7,\n",
       " 587,\n",
       " 2,\n",
       " 5058,\n",
       " 4058,\n",
       " 7,\n",
       " 568,\n",
       " 6452,\n",
       " 6641,\n",
       " 3604,\n",
       " 1328,\n",
       " 1354,\n",
       " 7,\n",
       " 7,\n",
       " 7]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp1_tokenizer.encode(text_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a88fa52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "622fce28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Percy <|unk|> and the <|unk|> Thief <|endoftext|> <|unk|>. It' s me. I was wondering if after all...\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp1_tokenizer.decode(hp1_tokenizer.encode(text_hp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a30071b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From comparing the decoded tokenized text above with \n",
    "# The original input text, we know that the training set \n",
    "# did not contain the words \"Hello\" and \"palace\"\n",
    "\n",
    "# Could use other tokens\n",
    "# [BOS] (beginning of sequence) Marks the start of a text/sequence\n",
    "# [EOS] (end of sequence): Positioned at the end of a text\n",
    "# and is useful for concatenating multiple unrelated texts.\n",
    "# [PAD] (padding): When training LLMs with a batch size larger\n",
    "# than one, the batch might contain texts of varying lengths. To ensure that \n",
    "# all the texts have the smae length the shorter texts\n",
    "# are padded using [PAD] token, up to length of the longest text in batch\n",
    "\n",
    "# Tokenizer used for GPT models does not need any of these \n",
    "# tokens mentioned but only uses an <|endoftext|> for simplicity\n",
    "\n",
    "# byte pair enconding tokenizer breaks down words \n",
    "# into subword units for tokenizer with GPT models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f0ae6",
   "metadata": {},
   "source": [
    "#### BYTE PAIR ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "619379d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# Use tiktoken github\n",
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d52ccf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(f'tiktoken version: {importlib.metadata.version(\"tiktoken\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0b44d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate BPE tokenizer from tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "hp1_tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3e9f709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271]\n"
     ]
    }
   ],
   "source": [
    "# Usage of tokenizer is similar to SimpleTokenzizerV2\n",
    "# We implemented previously via an encode method\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace\"\n",
    ")\n",
    "encoded_mapping = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
    "print(encoded_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58c72e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace\n"
     ]
    }
   ],
   "source": [
    "# Can then convert token IDs back into text using the decode\n",
    "# method. Similar to the SimpleTokenizerV2\n",
    "# Tokenizer is able to encode words that look wrong, like \n",
    "# someunknownPlace\n",
    "\n",
    "tokens = tokenizer.decode(encoded_mapping)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9d137f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two advantages so far.\n",
    "# 1. Reduces the amount of tokens we have\n",
    "# 2. Can deal with unknown words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6111628",
   "metadata": {},
   "source": [
    "### Can make two noteworthy observations based on the token IDs and decoded text above:\n",
    "\n",
    "#### 1. <|endoftext|> token is assigned to relatively large token ID (50256)\n",
    "#### 2. BPE tokenizer above encodes and decodes unknown words, can handle any unknown word.\n",
    "\n",
    "#### Algorithm underlying BPE breaks down words that aren't in its predefined vocab into smaller subwords. \n",
    "#### This enables it to handle out-of-vocabulary words.\n",
    "#### Because of BPE algorithm, if the tokenizer encounters an unfamilar word during tokenization, it can represent it as a sequence of subwords or characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2af08c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 80, 220, 959]\n",
      "Akwirq ier\n"
     ]
    }
   ],
   "source": [
    "# Proof that it works, even with gibberish\n",
    "encoded_mapping = tokenizer.encode(\"Akwirq ier\")\n",
    "print(encoded_mapping)\n",
    "\n",
    "tokens = tokenizer.decode(encoded_mapping)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4317ada9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 1047, 11, 9457, 11, 300, 1047, 11, 277, 1047, 11, 17666, 338, 11, 460, 470, 11, 6584, 470, 11, 460, 470, 11, 1839, 470, 11, 1028, 11, 22279, 11, 1276, 11, 17000, 11, 34297, 11, 427, 272, 470]\n",
      "tames, breaks, lames, fames, dont's, can't, shouldn't, can't, won't, against, lust, must, rust, fuss, shan't\n"
     ]
    }
   ],
   "source": [
    "# One more example with words that include apostrophes\n",
    "text = \"tames, breaks, lames, fames, dont's, can't, shouldn't, can't, won't, against, lust, must, rust, fuss, shan't\"\n",
    "encoded_mapping = tokenizer.encode(text)\n",
    "print(encoded_mapping)\n",
    "\n",
    "tokens = tokenizer.decode(encoded_mapping)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b6f84",
   "metadata": {},
   "source": [
    "### Creating Input-Target Pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8027c688",
   "metadata": {},
   "source": [
    "#### Implement a data loader that fetches input-target pairs using sliding window approach\n",
    "#### Tokenize the two stories first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33698f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Verdict encoder text length (tokens) is 5145\n",
      "Harry Potter Book 1 encoder text length (tokens) is 133860\n"
     ]
    }
   ],
   "source": [
    "# Store the book in a variable raw_text\n",
    "with open(\"books_text/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "with open(\"books_text/sorcerers-stone.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hp1_sorcerers_stone_text = f.read()\n",
    "enc_text_ver = tokenizer.encode(raw_text)\n",
    "enc_text_hp1 = tokenizer.encode(hp1_sorcerers_stone_text)\n",
    "print(f'The Verdict encoder text length (tokens) is {len(enc_text_ver)}')\n",
    "print(f'Harry Potter Book 1 encoder text length (tokens) is {len(enc_text_hp1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85a1306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [40, 367, 2885, 1464]\n",
      "y:     [367, 2885, 1464, 1807]\n"
     ]
    }
   ],
   "source": [
    "# x: [1,2,3,4], y: [2,3,4,5]\n",
    "# x:input, y:output\n",
    "# if [1] is the input, [2] should be output\n",
    "# if [1,2] is input, [3] should be output\n",
    "# if [1,2,3] is input, [4] should be output\n",
    "# if [1,2,3,4] is input. [5] should be output\n",
    "# Context size is how may words do we want to give as input\n",
    "# for model to predict next word\n",
    "\n",
    "context_size = 4 # length of input\n",
    "# Model is trained to look at sequence of 4 words (or tokens)\n",
    "# to predict next word in sequence.\n",
    "\n",
    "# For The Verdict sample\n",
    "x = enc_text_ver[:context_size]\n",
    "y = enc_text_ver[1:context_size+1]\n",
    "\n",
    "print(f'x: {x}')\n",
    "print(f'y:     {y}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "769ef1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_hp: [10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246, 13, 290, 9074, 13, 360, 1834, 1636, 11, 286, 1271]\n",
      "y_hp:      [16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246, 13, 290, 9074, 13, 360, 1834, 1636, 11, 286, 1271, 1440]\n"
     ]
    }
   ],
   "source": [
    "context_size_hp1 = 20\n",
    "x_hp = enc_text_hp1[:context_size_hp1]\n",
    "y_hp = enc_text_hp1[1:context_size_hp1+1]\n",
    "print(f'x_hp: {x_hp}')\n",
    "print(f'y_hp:      {y_hp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e42b434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40] ----> 367\n",
      "[40, 367] ----> 2885\n",
      "[40, 367, 2885] ----> 1464\n",
      "[40, 367, 2885, 1464] ----> 1807\n"
     ]
    }
   ],
   "source": [
    "# Processing inputs along with targets, we can create \n",
    "# next word prediction task as follows (The Verdict):\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_text_ver[:i]\n",
    "    desired = enc_text_ver[i]\n",
    "\n",
    "    print(context, \"---->\", desired)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f4f8f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10970] ----> 16494\n",
      "[10970, 16494] ----> 56\n",
      "[10970, 16494, 56] ----> 19494\n",
      "[10970, 16494, 56, 19494] ----> 406\n",
      "[10970, 16494, 56, 19494, 406] ----> 3824\n",
      "[10970, 16494, 56, 19494, 406, 3824] ----> 1961\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961] ----> 198\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198] ----> 198\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198] ----> 5246\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246] ----> 13\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246, 13] ----> 290\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246, 13, 290] ----> 9074\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246, 13, 290, 9074] ----> 13\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246, 13, 290, 9074, 13] ----> 360\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246, 13, 290, 9074, 13, 360] ----> 1834\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246, 13, 290, 9074, 13, 360, 1834] ----> 1636\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246, 13, 290, 9074, 13, 360, 1834, 1636] ----> 11\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246, 13, 290, 9074, 13, 360, 1834, 1636, 11] ----> 286\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246, 13, 290, 9074, 13, 360, 1834, 1636, 11, 286] ----> 1271\n",
      "[10970, 16494, 56, 19494, 406, 3824, 1961, 198, 198, 5246, 13, 290, 9074, 13, 360, 1834, 1636, 11, 286, 1271] ----> 1440\n"
     ]
    }
   ],
   "source": [
    "# Processing inputs along with targets, we can create \n",
    "# next word prediction task as follows (Harry Potter):\n",
    "for i in range(1, context_size_hp1+1):\n",
    "    context = enc_text_hp1[:i]\n",
    "    desired = enc_text_hp1[i]\n",
    "\n",
    "    print(context, \"---->\", desired)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ec14606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ---->  H\n",
      "I H ----> AD\n",
      "I HAD ---->  always\n",
      "I HAD always ---->  thought\n"
     ]
    }
   ],
   "source": [
    "# Repeat process but converting the IDs to tokens for Visualization \n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_text_ver[:i]\n",
    "    desired = enc_text_ver[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b4477b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ---->  BO\n",
      "THE BO ----> Y\n",
      "THE BOY ---->  WHO\n",
      "THE BOY WHO ---->  L\n",
      "THE BOY WHO L ----> IV\n",
      "THE BOY WHO LIV ----> ED\n",
      "THE BOY WHO LIVED ----> \n",
      "\n",
      "THE BOY WHO LIVED\n",
      " ----> \n",
      "\n",
      "THE BOY WHO LIVED\n",
      "\n",
      " ----> Mr\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr ----> .\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. ---->  and\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and ---->  Mrs\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs ----> .\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. ---->  D\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. D ----> urs\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Durs ----> ley\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Dursley ----> ,\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Dursley, ---->  of\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Dursley, of ---->  number\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Dursley, of number ---->  four\n"
     ]
    }
   ],
   "source": [
    "# For Harry Potter book 1:\n",
    "for i in range(1, context_size_hp1+1):\n",
    "    context = enc_text_hp1[:i]\n",
    "    desired = enc_text_hp1[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793fa76e",
   "metadata": {},
   "source": [
    "#### Created input-target pairs that we can turn into use for LLM training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07766756",
   "metadata": {},
   "source": [
    "#### Implement an efficient data loader that iterates over input dataset and returns inputs and targets as PyTorch tensors (multidimensional arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad31f1",
   "metadata": {},
   "source": [
    "#### We are interested in returning two tensors: An input tensor containing the text that LLM sees and a target tensor that includes the LLM to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd3f88",
   "metadata": {},
   "source": [
    "## Implementing a Data Loader\n",
    "#### Use PyTorch in built Datasets and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77ee35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.8.0-cp312-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "Successfully installed sympy-1.14.0 torch-2.8.0\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch\n",
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa226a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special = {\"<|endoftext|>\"})\n",
    "\n",
    "        # Use sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+1+max_length]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    # What DataLoader will be using\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a23065",
   "metadata": {},
   "source": [
    "#### GPTDatasetV1 Describes how indivdiual rows are fetched from dataset.\n",
    "#### Each row consists of a number of token IDs (based on max_length) assigned to an input_chunk tensor\n",
    "#### target_chunk tensor contains corresponding targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3927f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize tokenizer\n",
    "# Step 2: Create Dataset\n",
    "# Step 3: drop_last = True drops the last batch if it is shorder\n",
    "#         Than the specified batch_size to prevent \n",
    "#         loss spikes during training\n",
    "# Step 4: Number of CPU processes to use for preprocessing\n",
    "\n",
    "# Will help us doing parallel processing and can analyze \n",
    "# Multiple batch_sizes at a time\n",
    "# Batch_size = number of batches model processes at once before\n",
    "#              Updating parameters\n",
    "# num_workers = for parallel processing on multiple threads\n",
    "#               on CPU\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b8c55",
   "metadata": {},
   "source": [
    "#### Let's test dataloader with a batch size of 1 for an LLM with a context size of 4\n",
    "#### Purpose: Develop intuition of how GPTDatasetV1 class and the create_dataloader_v1 function work together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d6f99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the book in a variable raw_text\n",
    "with open(\"books_text/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "with open(\"books_text/sorcerers-stone.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hp1_sorcerers_stone_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a5c3c",
   "metadata": {},
   "source": [
    "#### Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "280bd603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1,shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d630df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[10970, 16494,    56, 19494]]), tensor([[16494,    56, 19494,   406]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader_hp1 = create_dataloader_v1(\n",
    "    hp1_sorcerers_stone_text, batch_size=1, max_length=4, stride=1,shuffle=False\n",
    ")\n",
    "\n",
    "data_iter_hp1 = iter(dataloader_hp1)\n",
    "first_batch_hp1 = next(data_iter_hp1)\n",
    "print(first_batch_hp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f6da5",
   "metadata": {},
   "source": [
    "#### first_batch variable contains two tensors: the first tensor stores the input token IDs and the second tensor stores the output token IDs\n",
    "#### Since max_length is set to 4, each of two tensors contain 4 token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea1a1489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c22d136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[16494,    56, 19494,   406]]), tensor([[   56, 19494,   406,  3824]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch_hp1 = next(data_iter_hp1)\n",
    "print(second_batch_hp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2961d",
   "metadata": {},
   "source": [
    "#### NOTE: Batch_size is a trade_off and a hyperparameter to experiment with when training LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# What happens when batch_size is more than 1?\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, \n",
    "    batch_size=8,\n",
    "    max_length=4,\n",
    "    stride=4,\n",
    "    shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Targets:\\n\", targets)\n",
    "# Model will process this batch before making parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b48f3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[10970, 16494,    56,  ..., 27034,    13, 14179],\n",
      "        [  373,  9074,    13,  ...,   198,   198, 14202],\n",
      "        [  286,   606,  6810,  ...,    13,   360,  1834],\n",
      "        ...,\n",
      "        [  355,   627,  1428,  ..., 14412,   373, 47207],\n",
      "        [   13,   679,   373,  ...,   290,   900,   572],\n",
      "        [  866,   262,   198,  ..., 25031, 11130,   261]])\n",
      "Targets:\n",
      " tensor([[16494,    56, 19494,  ...,    13, 14179,   373],\n",
      "        [ 9074,    13,   360,  ...,   198, 14202,   286],\n",
      "        [  606,  6810,   257,  ...,   360,  1834,  1636],\n",
      "        ...,\n",
      "        [  627,  1428,   618,  ...,   373, 47207,    13],\n",
      "        [  679,   373,  8179,  ...,   900,   572,   866],\n",
      "        [  262,   198, 25662,  ..., 11130,   261, 44906]])\n"
     ]
    }
   ],
   "source": [
    "# What happens when batch_size is more than 1 for HP?\n",
    "# When we try using parallel processers (# workers), Jupyter Notebooks throws an error, this is because,\n",
    "# On macOS, multiprocessing with fork can cause errors when num_workers > 0, especially in Jupyter or VSCode notebooks. \n",
    "# PyTorch tries to spawn subprocesses to load data, but the way Python handles multiprocessing on macOS is different \n",
    "# (it defaults to spawn instead of fork), which can break if the dataset or tokenizer isn’t picklable.\n",
    "dataloader_hp1 = create_dataloader_v1(\n",
    "    hp1_sorcerers_stone_text, \n",
    "    batch_size=16,\n",
    "    max_length=256,\n",
    "    stride=256,\n",
    "    shuffle=False)\n",
    "data_iter_hp1 = iter(dataloader_hp1)\n",
    "inputs_hp1, targets_hp1 = next(data_iter_hp1)\n",
    "print(\"Inputs:\\n\", inputs_hp1)\n",
    "print(\"Targets:\\n\", targets_hp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86630d1e",
   "metadata": {},
   "source": [
    "#### Increase stride to 4 so we can use dataset fully (we dont skip a word), but also avoid any overlap between the batches because increased overlap can lead to increased overfitting (for The Verdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole Input text:\n",
    "# \"In the heart of the city stood the old library, a relic from\n",
    "#  a bygone era. Its stone walls bore the mars of time, and ivy\n",
    "#  clung tightly to its facade...\"\n",
    "\n",
    "# People keep the stride length = to context length, so\n",
    "# We dont miss any words\n",
    "\n",
    "# Stride = 1: \n",
    "# Input of batch 1: \"In the heart of\"\n",
    "# Input of batch 2: \"the heart of the\"\n",
    "\n",
    "# Stride = 4:\n",
    "# Input of batch 1: \"In the heart of\"\n",
    "# Input of batch 2: \"the city stood the\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffbb4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
